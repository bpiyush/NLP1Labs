{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-aRiOgl4nHg"
   },
   "source": [
    "------\n",
    "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
    "\n",
    "------\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIZrAUx57vsM"
   },
   "source": [
    "Practical 1: Sentiment Detection in Movie Reviews\n",
    "========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4kXPMhyngZW"
   },
   "source": [
    "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
    "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
    "Each review is a **document** and consists of one or more sentences.\n",
    "\n",
    "To prepare yourself for this practical, you should\n",
    "have a look at a few of these texts to understand the difficulties of\n",
    "the task: how might one go about classifying the texts? You will write\n",
    "code that decides whether a movie review conveys positive or\n",
    "negative sentiment.\n",
    "\n",
    "Please make sure you have read the following paper:\n",
    "\n",
    ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
    "(2002). \n",
    "[Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
    "\n",
    "Bo Pang et al. introduced the movie review sentiment\n",
    "classification task, and the above paper was one of the first papers on\n",
    "the topic. The first version of your sentiment classifier will do\n",
    "something similar to Pang et al.'s system. If you have questions about it,\n",
    "you should resolve you doubts as soon as possible with your TA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hok-BFu9lGoK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "from subprocess import call\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sklearn as sk\n",
    "# from google.colab import drive\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "careEKj-mRpl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 2000 \n",
      "\n",
      "0 NEG 29\n",
      "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
      "1 NEG 11\n",
      "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
      "2 NEG 24\n",
      "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
      "3 NEG 19\n",
      "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
      "4 NEG 38\n",
      "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
      "\n",
      "Number of word types: 47743\n",
      "Number of word tokens: 1512359\n",
      "\n",
      "Most common tokens:\n",
      "         , :    77842\n",
      "       the :    75948\n",
      "         . :    59027\n",
      "         a :    37583\n",
      "       and :    35235\n",
      "        of :    33864\n",
      "        to :    31601\n",
      "        is :    25972\n",
      "        in :    21563\n",
      "        's :    18043\n",
      "        it :    15904\n",
      "      that :    15820\n",
      "     -rrb- :    11768\n",
      "     -lrb- :    11670\n",
      "        as :    11312\n",
      "      with :    10739\n",
      "       for :     9816\n",
      "       his :     9542\n",
      "      this :     9497\n",
      "      film :     9404\n"
     ]
    }
   ],
   "source": [
    "# file structure:\n",
    "# [\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
    "#   ..\n",
    "# ]\n",
    "# where `content` is a list of sentences, \n",
    "# with a sentence being a list of (token, pos_tag) pairs.\n",
    "\n",
    "\n",
    "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    reviews = json.load(f)\n",
    "\n",
    "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
    "\n",
    "def print_sentence_with_pos(s):\n",
    "    print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
    "\n",
    "for i, r in enumerate(reviews):\n",
    "    print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
    "    print_sentence_with_pos(r[\"content\"][0])\n",
    "    if i == 4: \n",
    "        break\n",
    "    \n",
    "c = Counter()\n",
    "for review in reviews:\n",
    "    for sentence in review[\"content\"]:\n",
    "        for token, pos_tag in sentence:\n",
    "            c[token.lower()] += 1\n",
    "\n",
    "print(\"\\nNumber of word types:\", len(c))\n",
    "print(\"Number of word tokens:\", sum(c.values()))\n",
    "\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in c.most_common(20):\n",
    "      print(\"%10s : %8d\" % (token, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ogq0Eq2hQglh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
     ]
    }
   ],
   "source": [
    "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    line_cnt = 0\n",
    "    for line in f:\n",
    "        print(line.strip())\n",
    "        line_cnt += 1\n",
    "        if line_cnt > 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features, overfitting, and the curse of dimensionality\n",
    "\n",
    "In the Bag-of-Words model, ideally we would like each distinct word in\n",
    "the text to be mapped to its own dimension in the output vector\n",
    "representation. However, real world text is messy, and we need to decide\n",
    "on what we consider to be a word. For example, is “`word`\" different\n",
    "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
    "definition, and the number of features explodes, while our algorithm\n",
    "fails to learn anything generalisable. Too lax, and we risk destroying\n",
    "our learning signal. In the following section, you will learn about\n",
    "confronting the feature sparsity and the overfitting problems as they\n",
    "occur in NLP classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming (1.5pts)\n",
    "\n",
    "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
    "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word', 'word', 'word']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# example usage\n",
    "list(map(stemmer.stem, [\"words\", \"Word\", \"word\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoazfxbNV5Lq"
   },
   "source": [
    "### N-grams (1.5pts)\n",
    "\n",
    "A simple way of retaining some of the word\n",
    "order information when using bag-of-words representations is to use **n-gram** features. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHjy3I7-qWiu"
   },
   "source": [
    "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
    "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    \"\"\"BoW-based feature encoder.\"\"\"\n",
    "    def __init__(self, n_grams=[1], stemmer=None, verbose=True, classes=[\"POS\", \"NEG\"]):\n",
    "        self.n_grams = n_grams\n",
    "        self.stemmer = stemmer\n",
    "        self.verbose = verbose\n",
    "        self.classes = classes\n",
    "    \n",
    "    def get_sentence_ngrams(self, sent: list):\n",
    "        joined_ngrams = [list(ngrams(sent, k)) for k in self.n_grams]\n",
    "        return list(itertools.chain.from_iterable(joined_ngrams))\n",
    "    \n",
    "    def get_ngram_freq_dict(self, document: list, use_pos=False):\n",
    "        document = np.concatenate(document)\n",
    "\n",
    "        # get all words in the document (in lowercase)\n",
    "        words = list(np.char.lower(document[:, 0]))\n",
    "        pos_tags = document[:, 1]\n",
    "\n",
    "        # apply stemming to the words\n",
    "        if self.stemmer is not None:\n",
    "            words = list(map(self.stemmer.stem, words))\n",
    "        \n",
    "        # apply pos tags as suffixes to words\n",
    "        if use_pos:\n",
    "            # document = np.concatenate(document)\n",
    "            # document[:, 0] = np.char.lower(document[:, 0])\n",
    "            words = np.char.add(np.char.add(np.array(words), \"_\"), pos_tags)\n",
    "            words = list(words)\n",
    "\n",
    "        # get collection of n-grams, for different n\n",
    "        n_grams = self.get_sentence_ngrams(words)\n",
    "        n_grams = np.array(n_grams, dtype=\"object\")\n",
    "\n",
    "        # get counts of n-grams\n",
    "        n_grams_unique, n_grams_counts = np.unique(n_grams, return_counts=True)\n",
    "        n_grams_freq_dict = dict(zip(n_grams_unique, n_grams_counts))\n",
    "        return n_grams_freq_dict\n",
    "    \n",
    "    def create_vocabulary(self, documents: list, labels: list, use_pos=False):\n",
    "        \"\"\"Create a combined vocabulary based on given n-grams, \\forall n.\"\"\"\n",
    "        num_docs = len(documents)\n",
    "        \n",
    "        iterator = tqdm(\n",
    "            range(num_docs),\n",
    "            desc=\"Creating vocabulary\",\n",
    "            bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}',\n",
    "        )\n",
    "        if not self.verbose:\n",
    "            iterator = range(num_docs)\n",
    "        \n",
    "        vocab = Counter()\n",
    "        class_wise_vocab = {c: Counter() for c in self.classes}\n",
    "        class_priors = {c: 0.0 for c in self.classes}\n",
    "        for i in iterator:\n",
    "            d, l = documents[i], labels[i]\n",
    "\n",
    "            # update class counts\n",
    "            class_priors[l] += (1.0 / num_docs)\n",
    "            \n",
    "            # update vocab\n",
    "            n_grams_freq_dict = self.get_ngram_freq_dict(d, use_pos=use_pos)\n",
    "            vocab.update(n_grams_freq_dict)\n",
    "            class_wise_vocab[l].update(n_grams_freq_dict)\n",
    "            \n",
    "        self.vocab = vocab\n",
    "        self.vocab_terms = np.array(list(vocab.keys()))\n",
    "        self.class_wise_vocab = class_wise_vocab\n",
    "        self.class_priors = class_priors\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    \n",
    "    def filter_vocab(self):\n",
    "        \"\"\"Removes words that occur only in one class but not others.\"\"\"\n",
    "        vocab_common = set.intersection(*[set(self.class_wise_vocab[c].keys()) for c in self.classes])\n",
    "        self.vocab_size = len(vocab_common)\n",
    "        for c in self.classes:\n",
    "            vocab_class = {w:v for w, v in self.class_wise_vocab[c].items() if w in vocab_common}\n",
    "            self.class_wise_vocab[c] = vocab_class\n",
    "\n",
    "    def encode(self, document: list):\n",
    "        \"\"\"Encodes a document (list of sentences) into a BoW representation.\"\"\"\n",
    "\n",
    "        assert hasattr(self, \"vocab\"), \"Vocabulary has not been created!\"\n",
    "\n",
    "        n_grams_freq_dict = self.get_ngram_freq_dict(document)\n",
    "        n_grams_freq_dict = {k:v for k, v in n_grams_freq_dict.items() if k in self.vocab_terms}\n",
    "        \n",
    "        \n",
    "        words_in_doc = np.array(list(n_grams_freq_dict.keys()))\n",
    "        word_freqs_in_doc = np.array(list(n_grams_freq_dict.values()))\n",
    "        \n",
    "        bow_vector = np.zeros(len(self.vocab))\n",
    "        indices = np.in1d(self.vocab_terms, words_in_doc)\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        bow_vector[np.where(indices == True)] = word_freqs_in_doc\n",
    "        \n",
    "\n",
    "#         bow_vector = np.zeros(len(self.vocab))\n",
    "#         for key, count in n_grams_freq_dict.items():\n",
    "#             bow_vector[self.vocab_terms.index(key)] = count\n",
    "\n",
    "        return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \"\"\"Implements the NBClassifier.\"\"\"\n",
    "    def __init__(self, classes, n_grams=[1], stemmer=None, smoothing_kappa=0.0, filter_vocab=False, use_pos=False):\n",
    "        self.bow = BagOfWords(n_grams=n_grams, classes=classes, stemmer=stemmer)\n",
    "        self.smoothing_kappa = smoothing_kappa\n",
    "        self.classes = classes\n",
    "        self.filter_vocab = filter_vocab\n",
    "        self.use_pos = use_pos\n",
    "    \n",
    "    def train(self, documents: list, labels: list):\n",
    "        assert len(documents) == len(labels)\n",
    "        assert set(np.unique(labels)) == set(self.classes)\n",
    "\n",
    "        self.bow.create_vocabulary(documents, labels, use_pos=self.use_pos)\n",
    "        print(f\"Training finished with vocabulary of size {len(self.bow.vocab)}.\")\n",
    "\n",
    "        if self.filter_vocab:\n",
    "            self.bow.filter_vocab()\n",
    "            print(f\"Filtered vocabulary. Size of new vocabulary: {self.bow.vocab_size}\")\n",
    "\n",
    "    def check_word_in_vocab(self, word):\n",
    "        for c in self.classes:\n",
    "            if word not in self.bow.class_wise_vocab[c]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def predict(self, documents: list):\n",
    "        \"\"\"Predicts class label for each of the given documents.\"\"\"\n",
    "        num_docs = len(documents)\n",
    "        \n",
    "        class_wise_sum = {c: sum(list(self.bow.class_wise_vocab[c].values())) for c in self.classes}\n",
    "        \n",
    "        iterator = tqdm(\n",
    "            range(num_docs),\n",
    "            desc=\"Evaluating\",\n",
    "            bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}',\n",
    "        )\n",
    "        predictions = []\n",
    "        for i in iterator:\n",
    "            d = documents[i]\n",
    "            ngram_frequency_dict = self.bow.get_ngram_freq_dict(d, use_pos=self.use_pos)\n",
    "\n",
    "\n",
    "            score = {k: np.log(self.bow.class_priors[k]) for k in self.classes}\n",
    "            for word in ngram_frequency_dict:\n",
    "                if self.check_word_in_vocab(word):\n",
    "                    for c in self.classes:\n",
    "                        count_word_in_c = self.bow.class_wise_vocab[c][word]\n",
    "                        count_all_words_in_c = class_wise_sum[c]\n",
    "                        nume = count_word_in_c + self.smoothing_kappa\n",
    "                        deno = count_all_words_in_c + self.smoothing_kappa * len(self.bow.vocab)\n",
    "                        score[c] += np.log(nume/deno)\n",
    "        \n",
    "            predicted_class = max(score, key=score.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_based_on_pos_tags(documents, valid_tags=[\"NN\", \"VB\", \"JJ\", \"RB\"]):\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        # sent_lens = [len(x) for x in d]\n",
    "        # sent_lens = np.cumsum(sent_len)[:-1]\n",
    "        \n",
    "        doc = np.concatenate(d)\n",
    "        idx = np.in1d(doc[:, 1], np.array(valid_tags))\n",
    "        doc = doc[idx]\n",
    "        filtered_docs.append([doc.tolist()])\n",
    "    \n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true: list, y_pred: list):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.mean((y_true == y_pred).astype(int))\n",
    "\n",
    "\n",
    "def train_and_evaluate_clf(clf, train_idx, test_idx, filter_on_tags=False):\n",
    "    train_documents = [reviews[i][\"content\"] for i in train_idx]\n",
    "    train_labels = [reviews[i][\"sentiment\"] for i in train_idx]\n",
    "\n",
    "    if filter_on_tags:\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        train_documents = filter_based_on_pos_tags(train_documents)\n",
    "\n",
    "    test_documents = [reviews[i][\"content\"] for i in test_idx]\n",
    "    test_labels = [reviews[i][\"sentiment\"] for i in test_idx]\n",
    "\n",
    "    if filter_on_tags:\n",
    "        test_documents = filter_based_on_pos_tags(test_documents)\n",
    "\n",
    "    # train the classifier\n",
    "    clf.train(train_documents, train_labels)\n",
    "\n",
    "    # compute train accuracy\n",
    "    acc = compute_accuracy(train_labels, clf.predict(train_documents))\n",
    "    print(f\"Obtained accuracy using NaiveBayesClassifier on train set: {acc:.3f}\")\n",
    "\n",
    "    # compute test accuracy\n",
    "    acc = compute_accuracy(test_labels, clf.predict(test_documents))\n",
    "    print(f\"Obtained accuracy using NaiveBayesClassifier on test set: {acc:.3f}\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:02<00:00, 759.52it/s]                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with vocabulary of size 45348.\n",
      "Filtered vocabulary. Size of new vocabulary: 18799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 1800/1800 [00:05<00:00, 334.28it/s]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on train set: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 200/200 [00:00<00:00, 331.12it/s]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on test set: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into training and testing data\n",
    "cv = np.array([reviews[i][\"cv\"] for i in range(len(reviews))])\n",
    "labels = np.array([reviews[i][\"sentiment\"] for i in range(len(reviews))])\n",
    "\n",
    "train_idx = np.where(cv < 900)[0]\n",
    "test_idx = np.where(cv >= 900)[0]\n",
    "\n",
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], filter_vocab=True)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 990/990 [00:01<00:00, 742.09it/s]                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with vocabulary of size 34662.\n",
      "Filtered vocabulary. Size of new vocabulary: 7351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 990/990 [00:02<00:00, 351.76it/s]                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on train set: 0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 110/110 [00:00<00:00, 356.83it/s]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on test set: 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9181818181818182"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond = ((cv < 90) * (labels == \"NEG\")) + ((labels == \"POS\") * (cv < 900))\n",
    "train_idx = [i for i, value in enumerate(cond) if value]\n",
    "\n",
    "cond = ((cv >= 900) * (cv <= 909) * (labels == \"NEG\")) + ((labels == \"POS\") * (cv >= 900))\n",
    "test_idx = [i for i, value in enumerate(cond) if value]\n",
    "\n",
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], filter_vocab=True)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:02<00:00, 784.51it/s]                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with vocabulary of size 45348.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 1800/1800 [00:05<00:00, 328.56it/s]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on train set: 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 200/200 [00:00<00:00, 314.18it/s]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on test set: 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into training and testing data\n",
    "train_idx = np.where(cv < 900)[0]\n",
    "test_idx = np.where(cv >= 900)[0]\n",
    "\n",
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], smoothing_kappa=1.0)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:02<00:00, 767.34it/s]                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with vocabulary of size 45348.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 1800/1800 [00:05<00:00, 328.87it/s]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on train set: 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 200/200 [00:00<00:00, 329.05it/s]                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on test set: 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:11<00:00, 152.27it/s]                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with vocabulary of size 32404.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 1800/1800 [00:15<00:00, 116.40it/s]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on train set: 0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 200/200 [00:01<00:00, 114.12it/s]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on test set: 0.860\n",
      ":::: Vocabulary size ::::\n",
      ":: Without stemming:\t 45348\n",
      ":: With stemming:\t 32404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "train_idx = np.where(cv < 900)[0]\n",
    "test_idx = np.where(cv >= 900)[0]\n",
    "\n",
    "# without stemming\n",
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], stemmer=None, smoothing_kappa=1.0)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx)\n",
    "vocab_size_wout_stem = clf.bow.vocab_size\n",
    "\n",
    "# with stemming\n",
    "stemmer = PorterStemmer()\n",
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], stemmer=stemmer, smoothing_kappa=1.0)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx)\n",
    "vocab_size_with_stem = clf.bow.vocab_size\n",
    "\n",
    "print(\":::: Vocabulary size ::::\")\n",
    "print(f\":: Without stemming:\\t {vocab_size_wout_stem}\")\n",
    "print(f\":: With stemming:\\t {vocab_size_with_stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing data\n",
    "train_idx = np.where(cv < 900)[0]\n",
    "test_idx = np.where(cv >= 900)[0]\n",
    "\n",
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], n_grams=[1, 2], filter_vocab=True)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:07<00:00, 228.31it/s]                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with vocabulary of size 1416686.\n",
      "Filtered vocabulary. Size of new vocabulary: 164071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 1800/1800 [00:16<00:00, 107.72it/s]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on train set: 0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 200/200 [00:01<00:00, 111.59it/s]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on test set: 0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], n_grams=[1, 2, 3], filter_vocab=True)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BagOfWords(n_grams=[1], stemmer=stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I',), ('am',), ('jongen',)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.get_sentence_ngrams([\"I\", \"am\", \"jongen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating vocabulary:   0%|                    | 0/1800 [00:00<?, ?it/s]                                                                                  \u001b[A\n",
      "Creating vocabulary:   1%|▏                   | 12/1800 [00:00<00:15, 117.62it/s]                                                                        \u001b[A\n",
      "Creating vocabulary:   2%|▎                   | 27/1800 [00:00<00:13, 135.41it/s]                                                                        \u001b[A\n",
      "Creating vocabulary:   2%|▍                   | 43/1800 [00:00<00:12, 144.21it/s]                                                                        \u001b[A\n",
      "Creating vocabulary:   3%|▋                   | 62/1800 [00:00<00:11, 157.87it/s]                                                                        \u001b[A\n",
      "Creating vocabulary:   4%|▊                   | 78/1800 [00:00<00:11, 156.11it/s]                                                                        \u001b[A\n",
      "Creating vocabulary:   5%|█                   | 95/1800 [00:00<00:10, 157.73it/s]                                                                        \u001b[A\n",
      "Creating vocabulary:   6%|█▏                  | 111/1800 [00:00<00:11, 150.32it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:   7%|█▍                  | 127/1800 [00:00<00:10, 152.77it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:   8%|█▌                  | 143/1800 [00:00<00:11, 147.20it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:   9%|█▊                  | 159/1800 [00:01<00:11, 149.11it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  10%|█▉                  | 177/1800 [00:01<00:10, 157.35it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  11%|██▏                 | 193/1800 [00:01<00:10, 152.92it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  12%|██▎                 | 211/1800 [00:01<00:10, 158.21it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  13%|██▌                 | 228/1800 [00:01<00:09, 160.46it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  14%|██▋                 | 245/1800 [00:01<00:09, 156.26it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  14%|██▉                 | 261/1800 [00:01<00:10, 152.72it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  15%|███                 | 277/1800 [00:01<00:10, 147.21it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  16%|███▎                | 296/1800 [00:01<00:09, 158.88it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  17%|███▍                | 313/1800 [00:02<00:09, 156.45it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  18%|███▋                | 329/1800 [00:02<00:09, 154.88it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  19%|███▊                | 347/1800 [00:02<00:09, 159.87it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  20%|████                | 364/1800 [00:02<00:08, 161.94it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  21%|████▏               | 381/1800 [00:02<00:08, 157.72it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  22%|████▍               | 401/1800 [00:02<00:08, 167.71it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  23%|████▋               | 418/1800 [00:02<00:08, 158.10it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  24%|████▊               | 434/1800 [00:02<00:09, 148.35it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  25%|█████               | 453/1800 [00:02<00:08, 158.91it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  26%|█████▏              | 470/1800 [00:03<00:08, 158.85it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  27%|█████▍              | 487/1800 [00:03<00:08, 153.08it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  28%|█████▌              | 503/1800 [00:03<00:08, 153.03it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  29%|█████▊              | 522/1800 [00:03<00:07, 161.82it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  30%|█████▉              | 539/1800 [00:03<00:08, 156.46it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  31%|██████▏             | 555/1800 [00:03<00:07, 156.29it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  32%|██████▎             | 571/1800 [00:03<00:07, 156.63it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  33%|██████▌             | 589/1800 [00:03<00:07, 161.09it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  34%|██████▋             | 606/1800 [00:03<00:07, 162.20it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  35%|██████▉             | 623/1800 [00:04<00:07, 155.74it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  36%|███████             | 639/1800 [00:04<00:07, 150.32it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  36%|███████▎            | 655/1800 [00:04<00:07, 146.86it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  37%|███████▍            | 674/1800 [00:04<00:07, 157.94it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  38%|███████▋            | 691/1800 [00:04<00:06, 158.87it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  39%|███████▉            | 710/1800 [00:04<00:06, 163.54it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  40%|████████            | 727/1800 [00:04<00:06, 161.39it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  41%|████████▎           | 744/1800 [00:04<00:06, 158.34it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  42%|████████▍           | 760/1800 [00:04<00:06, 151.49it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  43%|████████▌           | 776/1800 [00:05<00:07, 146.11it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  44%|████████▊           | 793/1800 [00:05<00:06, 151.84it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  45%|█████████           | 810/1800 [00:05<00:06, 156.78it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  46%|█████████▏          | 826/1800 [00:05<00:06, 156.91it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  47%|█████████▎          | 842/1800 [00:05<00:06, 154.59it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  48%|█████████▌          | 860/1800 [00:05<00:05, 160.66it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  49%|█████████▋          | 877/1800 [00:05<00:05, 159.38it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  50%|█████████▉          | 894/1800 [00:05<00:05, 157.34it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  51%|██████████          | 911/1800 [00:05<00:05, 159.79it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  52%|██████████▎         | 928/1800 [00:05<00:05, 158.68it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  52%|██████████▍         | 944/1800 [00:06<00:05, 146.38it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  53%|██████████▋         | 961/1800 [00:06<00:05, 149.39it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  54%|██████████▊         | 977/1800 [00:06<00:05, 146.58it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  55%|███████████         | 995/1800 [00:06<00:05, 153.74it/s]                                                                       \u001b[A\n",
      "Creating vocabulary:  56%|███████████▏        | 1011/1800 [00:06<00:05, 151.98it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  57%|███████████▍        | 1027/1800 [00:06<00:05, 145.13it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  58%|███████████▌        | 1044/1800 [00:06<00:05, 149.73it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  59%|███████████▊        | 1060/1800 [00:06<00:04, 150.14it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  60%|███████████▉        | 1076/1800 [00:06<00:04, 146.24it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  61%|████████████▏       | 1094/1800 [00:07<00:04, 155.08it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  62%|████████████▎       | 1111/1800 [00:07<00:04, 158.94it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  63%|████████████▌       | 1129/1800 [00:07<00:04, 164.55it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  64%|████████████▋       | 1146/1800 [00:07<00:03, 164.70it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  65%|████████████▉       | 1163/1800 [00:07<00:04, 150.97it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  66%|█████████████       | 1179/1800 [00:07<00:04, 147.91it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  66%|█████████████▎      | 1196/1800 [00:07<00:03, 152.70it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  67%|█████████████▍      | 1212/1800 [00:07<00:04, 144.94it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  68%|█████████████▋      | 1228/1800 [00:07<00:03, 143.80it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  69%|█████████████▊      | 1243/1800 [00:08<00:03, 140.98it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  70%|██████████████      | 1261/1800 [00:08<00:03, 150.30it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  71%|██████████████▏     | 1278/1800 [00:08<00:03, 154.71it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  72%|██████████████▍     | 1295/1800 [00:08<00:03, 158.28it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  73%|██████████████▌     | 1311/1800 [00:08<00:03, 154.36it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  74%|██████████████▋     | 1327/1800 [00:08<00:03, 150.50it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  75%|██████████████▉     | 1343/1800 [00:08<00:03, 139.20it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  76%|███████████████     | 1360/1800 [00:08<00:03, 146.41it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  76%|███████████████▎    | 1375/1800 [00:08<00:02, 146.14it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  77%|███████████████▍    | 1391/1800 [00:09<00:02, 148.94it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  78%|███████████████▋    | 1407/1800 [00:09<00:02, 138.21it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  79%|███████████████▊    | 1422/1800 [00:09<00:02, 132.55it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  80%|███████████████▉    | 1438/1800 [00:09<00:02, 139.62it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  81%|████████████████▏   | 1453/1800 [00:09<00:02, 134.91it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  82%|████████████████▎   | 1467/1800 [00:09<00:02, 132.65it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  82%|████████████████▍   | 1482/1800 [00:09<00:02, 136.38it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  83%|████████████████▋   | 1498/1800 [00:09<00:02, 135.16it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  84%|████████████████▊   | 1512/1800 [00:09<00:02, 133.78it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  85%|████████████████▉   | 1526/1800 [00:10<00:02, 133.36it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  86%|█████████████████   | 1540/1800 [00:10<00:02, 129.36it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  86%|█████████████████▎  | 1554/1800 [00:10<00:01, 130.80it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  87%|█████████████████▍  | 1568/1800 [00:10<00:01, 130.86it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  88%|█████████████████▌  | 1582/1800 [00:10<00:01, 128.08it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  89%|█████████████████▋  | 1597/1800 [00:10<00:01, 131.83it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  90%|█████████████████▉  | 1611/1800 [00:10<00:01, 128.16it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  90%|██████████████████  | 1628/1800 [00:10<00:01, 138.77it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  91%|██████████████████▏ | 1642/1800 [00:10<00:01, 131.50it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  92%|██████████████████▍ | 1659/1800 [00:11<00:00, 141.47it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  93%|██████████████████▌ | 1674/1800 [00:11<00:00, 139.54it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  94%|██████████████████▊ | 1690/1800 [00:11<00:00, 143.11it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  95%|██████████████████▉ | 1705/1800 [00:11<00:00, 139.21it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  96%|███████████████████ | 1719/1800 [00:11<00:00, 133.82it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  96%|███████████████████▎| 1736/1800 [00:11<00:00, 143.34it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  97%|███████████████████▍| 1752/1800 [00:11<00:00, 147.53it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  98%|███████████████████▋| 1767/1800 [00:11<00:00, 146.95it/s]                                                                      \u001b[A\n",
      "Creating vocabulary:  99%|███████████████████▊| 1782/1800 [00:11<00:00, 145.80it/s]                                                                      \u001b[A\n",
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:12<00:00, 149.35it/s]                                                                      \u001b[A\n"
     ]
    }
   ],
   "source": [
    "train_documents = [reviews[i][\"content\"] for i in train_idx]\n",
    "train_labels = [reviews[i][\"sentiment\"] for i in train_idx]\n",
    "\n",
    "test_documents = [reviews[i][\"content\"] for i in test_idx]\n",
    "test_labels = [reviews[i][\"sentiment\"] for i in test_idx]\n",
    "\n",
    "bow.create_vocabulary(train_documents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32404, 23501, 22229)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow.vocab), len(bow.class_wise_vocab[\"POS\"]), len(bow.class_wise_vocab[\"NEG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('copiou', 2),\n",
       " ('copper', 4),\n",
       " ('could', 626),\n",
       " ('cours', 282),\n",
       " ('crack', 34),\n",
       " ('crazi', 42),\n",
       " ('creat', 308),\n",
       " ('creepi', 49),\n",
       " ('crime', 116),\n",
       " ('cring', 5)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bow.class_wise_vocab[\"POS\"].items())[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding train documents: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1800/1800 [00:37<00:00, 48.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X = [bow.encode(d) for d in tqdm(train_documents, desc=\"Encoding train documents\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding test documents:   0%|                                                                                                   | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "Encoding test documents:   0%|▍                                                                                          | 1/200 [00:00<00:22,  8.72it/s]\u001b[A\n",
      "Encoding test documents:   2%|█▎                                                                                         | 3/200 [00:00<00:15, 13.04it/s]\u001b[A\n",
      "Encoding test documents:   3%|██▋                                                                                        | 6/200 [00:00<00:12, 15.56it/s]\u001b[A\n",
      "Encoding test documents:   4%|███▋                                                                                       | 8/200 [00:00<00:13, 14.63it/s]\u001b[A\n",
      "Encoding test documents:   5%|████▌                                                                                     | 10/200 [00:00<00:13, 13.91it/s]\u001b[A\n",
      "Encoding test documents:   6%|█████▍                                                                                    | 12/200 [00:00<00:13, 13.66it/s]\u001b[A\n",
      "Encoding test documents:   7%|██████▎                                                                                   | 14/200 [00:01<00:14, 13.12it/s]\u001b[A\n",
      "Encoding test documents:   8%|███████▏                                                                                  | 16/200 [00:01<00:13, 13.62it/s]\u001b[A\n",
      "Encoding test documents:   9%|████████                                                                                  | 18/200 [00:01<00:13, 13.80it/s]\u001b[A\n",
      "Encoding test documents:  10%|█████████                                                                                 | 20/200 [00:01<00:13, 13.80it/s]\u001b[A\n",
      "Encoding test documents:  11%|█████████▉                                                                                | 22/200 [00:01<00:13, 13.27it/s]\u001b[A\n",
      "Encoding test documents:  12%|██████████▊                                                                               | 24/200 [00:01<00:12, 13.63it/s]\u001b[A\n",
      "Encoding test documents:  13%|███████████▋                                                                              | 26/200 [00:01<00:12, 14.05it/s]\u001b[A\n",
      "Encoding test documents:  14%|████████████▌                                                                             | 28/200 [00:02<00:13, 12.37it/s]\u001b[A\n",
      "Encoding test documents:  15%|█████████████▌                                                                            | 30/200 [00:02<00:13, 12.95it/s]\u001b[A\n",
      "Encoding test documents:  16%|██████████████▍                                                                           | 32/200 [00:02<00:12, 13.98it/s]\u001b[A\n",
      "Encoding test documents:  17%|███████████████▎                                                                          | 34/200 [00:02<00:13, 12.62it/s]\u001b[A\n",
      "Encoding test documents:  18%|████████████████▏                                                                         | 36/200 [00:02<00:13, 12.22it/s]\u001b[A\n",
      "Encoding test documents:  19%|█████████████████                                                                         | 38/200 [00:02<00:12, 13.21it/s]\u001b[A\n",
      "Encoding test documents:  20%|██████████████████                                                                        | 40/200 [00:02<00:11, 13.75it/s]\u001b[A\n",
      "Encoding test documents:  21%|██████████████████▉                                                                       | 42/200 [00:03<00:10, 14.90it/s]\u001b[A\n",
      "Encoding test documents:  22%|███████████████████▊                                                                      | 44/200 [00:03<00:10, 14.88it/s]\u001b[A\n",
      "Encoding test documents:  23%|████████████████████▋                                                                     | 46/200 [00:03<00:10, 14.52it/s]\u001b[A\n",
      "Encoding test documents:  24%|█████████████████████▌                                                                    | 48/200 [00:03<00:10, 15.17it/s]\u001b[A\n",
      "Encoding test documents:  25%|██████████████████████▌                                                                   | 50/200 [00:03<00:10, 14.24it/s]\u001b[A\n",
      "Encoding test documents:  26%|███████████████████████▍                                                                  | 52/200 [00:03<00:11, 12.81it/s]\u001b[A\n",
      "Encoding test documents:  27%|████████████████████████▎                                                                 | 54/200 [00:03<00:11, 13.11it/s]\u001b[A\n",
      "Encoding test documents:  28%|█████████████████████████▏                                                                | 56/200 [00:04<00:10, 13.68it/s]\u001b[A\n",
      "Encoding test documents:  29%|██████████████████████████                                                                | 58/200 [00:04<00:11, 12.86it/s]\u001b[A\n",
      "Encoding test documents:  30%|███████████████████████████                                                               | 60/200 [00:04<00:10, 13.42it/s]\u001b[A\n",
      "Encoding test documents:  31%|███████████████████████████▉                                                              | 62/200 [00:04<00:09, 14.02it/s]\u001b[A\n",
      "Encoding test documents:  32%|████████████████████████████▊                                                             | 64/200 [00:04<00:09, 13.71it/s]\u001b[A\n",
      "Encoding test documents:  33%|█████████████████████████████▋                                                            | 66/200 [00:04<00:09, 13.98it/s]\u001b[A\n",
      "Encoding test documents:  34%|██████████████████████████████▌                                                           | 68/200 [00:04<00:09, 13.85it/s]\u001b[A\n",
      "Encoding test documents:  35%|███████████████████████████████▍                                                          | 70/200 [00:05<00:09, 14.22it/s]\u001b[A\n",
      "Encoding test documents:  36%|████████████████████████████████▍                                                         | 72/200 [00:05<00:09, 13.35it/s]\u001b[A\n",
      "Encoding test documents:  37%|█████████████████████████████████▎                                                        | 74/200 [00:05<00:10, 12.51it/s]\u001b[A\n",
      "Encoding test documents:  38%|██████████████████████████████████▏                                                       | 76/200 [00:05<00:10, 12.12it/s]\u001b[A\n",
      "Encoding test documents:  39%|███████████████████████████████████                                                       | 78/200 [00:05<00:09, 13.16it/s]\u001b[A\n",
      "Encoding test documents:  40%|████████████████████████████████████                                                      | 80/200 [00:05<00:09, 13.29it/s]\u001b[A\n",
      "Encoding test documents:  41%|████████████████████████████████████▉                                                     | 82/200 [00:06<00:09, 13.11it/s]\u001b[A\n",
      "Encoding test documents:  42%|█████████████████████████████████████▊                                                    | 84/200 [00:06<00:08, 14.16it/s]\u001b[A\n",
      "Encoding test documents:  43%|██████████████████████████████████████▋                                                   | 86/200 [00:06<00:08, 14.21it/s]\u001b[A\n",
      "Encoding test documents:  44%|███████████████████████████████████████▌                                                  | 88/200 [00:06<00:08, 14.00it/s]\u001b[A\n",
      "Encoding test documents:  45%|████████████████████████████████████████▌                                                 | 90/200 [00:06<00:07, 14.06it/s]\u001b[A\n",
      "Encoding test documents:  46%|█████████████████████████████████████████▍                                                | 92/200 [00:06<00:07, 14.72it/s]\u001b[A\n",
      "Encoding test documents:  47%|██████████████████████████████████████████▎                                               | 94/200 [00:06<00:07, 14.49it/s]\u001b[A\n",
      "Encoding test documents:  48%|███████████████████████████████████████████▏                                              | 96/200 [00:07<00:08, 12.97it/s]\u001b[A\n",
      "Encoding test documents:  49%|████████████████████████████████████████████                                              | 98/200 [00:07<00:07, 12.90it/s]\u001b[A\n",
      "Encoding test documents:  50%|████████████████████████████████████████████▌                                            | 100/200 [00:07<00:07, 13.57it/s]\u001b[A\n",
      "Encoding test documents:  51%|█████████████████████████████████████████████▍                                           | 102/200 [00:07<00:06, 14.31it/s]\u001b[A\n",
      "Encoding test documents:  52%|██████████████████████████████████████████████▎                                          | 104/200 [00:07<00:07, 12.91it/s]\u001b[A\n",
      "Encoding test documents:  53%|███████████████████████████████████████████████▏                                         | 106/200 [00:07<00:07, 13.32it/s]\u001b[A\n",
      "Encoding test documents:  54%|████████████████████████████████████████████████                                         | 108/200 [00:07<00:06, 14.28it/s]\u001b[A\n",
      "Encoding test documents:  55%|████████████████████████████████████████████████▉                                        | 110/200 [00:08<00:06, 12.89it/s]\u001b[A\n",
      "Encoding test documents:  56%|█████████████████████████████████████████████████▊                                       | 112/200 [00:08<00:07, 12.54it/s]\u001b[A\n",
      "Encoding test documents:  57%|██████████████████████████████████████████████████▋                                      | 114/200 [00:08<00:06, 12.79it/s]\u001b[A\n",
      "Encoding test documents:  58%|███████████████████████████████████████████████████▌                                     | 116/200 [00:08<00:05, 14.01it/s]\u001b[A\n",
      "Encoding test documents:  59%|████████████████████████████████████████████████████▌                                    | 118/200 [00:08<00:05, 14.26it/s]\u001b[A\n",
      "Encoding test documents:  60%|█████████████████████████████████████████████████████▍                                   | 120/200 [00:08<00:05, 14.48it/s]\u001b[A\n",
      "Encoding test documents:  61%|██████████████████████████████████████████████████████▎                                  | 122/200 [00:08<00:05, 14.85it/s]\u001b[A\n",
      "Encoding test documents:  62%|███████████████████████████████████████████████████████▏                                 | 124/200 [00:09<00:05, 13.66it/s]\u001b[A\n",
      "Encoding test documents:  63%|████████████████████████████████████████████████████████                                 | 126/200 [00:09<00:05, 13.71it/s]\u001b[A\n",
      "Encoding test documents:  64%|████████████████████████████████████████████████████████▉                                | 128/200 [00:09<00:05, 13.66it/s]\u001b[A\n",
      "Encoding test documents:  65%|█████████████████████████████████████████████████████████▊                               | 130/200 [00:09<00:05, 12.58it/s]\u001b[A\n",
      "Encoding test documents:  66%|██████████████████████████████████████████████████████████▋                              | 132/200 [00:09<00:05, 13.05it/s]\u001b[A\n",
      "Encoding test documents:  67%|███████████████████████████████████████████████████████████▋                             | 134/200 [00:09<00:05, 12.89it/s]\u001b[A\n",
      "Encoding test documents:  68%|████████████████████████████████████████████████████████████▌                            | 136/200 [00:10<00:05, 12.07it/s]\u001b[A\n",
      "Encoding test documents:  69%|█████████████████████████████████████████████████████████████▍                           | 138/200 [00:10<00:05, 11.58it/s]\u001b[A\n",
      "Encoding test documents:  70%|██████████████████████████████████████████████████████████████▎                          | 140/200 [00:10<00:04, 13.19it/s]\u001b[A\n",
      "Encoding test documents:  71%|███████████████████████████████████████████████████████████████▏                         | 142/200 [00:10<00:04, 13.33it/s]\u001b[A\n",
      "Encoding test documents:  72%|████████████████████████████████████████████████████████████████                         | 144/200 [00:10<00:03, 14.10it/s]\u001b[A\n",
      "Encoding test documents:  73%|████████████████████████████████████████████████████████████████▉                        | 146/200 [00:10<00:04, 13.48it/s]\u001b[A\n",
      "Encoding test documents:  74%|█████████████████████████████████████████████████████████████████▊                       | 148/200 [00:11<00:04, 11.83it/s]\u001b[A\n",
      "Encoding test documents:  75%|██████████████████████████████████████████████████████████████████▊                      | 150/200 [00:11<00:04, 12.17it/s]\u001b[A\n",
      "Encoding test documents:  76%|███████████████████████████████████████████████████████████████████▋                     | 152/200 [00:11<00:03, 12.83it/s]\u001b[A\n",
      "Encoding test documents:  77%|████████████████████████████████████████████████████████████████████▌                    | 154/200 [00:11<00:03, 12.88it/s]\u001b[A\n",
      "Encoding test documents:  78%|█████████████████████████████████████████████████████████████████████▍                   | 156/200 [00:11<00:03, 12.35it/s]\u001b[A\n",
      "Encoding test documents:  79%|██████████████████████████████████████████████████████████████████████▎                  | 158/200 [00:11<00:03, 12.70it/s]\u001b[A\n",
      "Encoding test documents:  80%|███████████████████████████████████████████████████████████████████████▏                 | 160/200 [00:11<00:02, 13.38it/s]\u001b[A\n",
      "Encoding test documents:  81%|████████████████████████████████████████████████████████████████████████                 | 162/200 [00:12<00:02, 13.03it/s]\u001b[A\n",
      "Encoding test documents:  82%|████████████████████████████████████████████████████████████████████████▉                | 164/200 [00:12<00:02, 13.23it/s]\u001b[A\n",
      "Encoding test documents:  83%|█████████████████████████████████████████████████████████████████████████▊               | 166/200 [00:12<00:02, 12.97it/s]\u001b[A\n",
      "Encoding test documents:  84%|██████████████████████████████████████████████████████████████████████████▊              | 168/200 [00:12<00:02, 12.43it/s]\u001b[A\n",
      "Encoding test documents:  85%|███████████████████████████████████████████████████████████████████████████▋             | 170/200 [00:12<00:02, 12.79it/s]\u001b[A\n",
      "Encoding test documents:  86%|████████████████████████████████████████████████████████████████████████████▌            | 172/200 [00:12<00:02, 12.71it/s]\u001b[A\n",
      "Encoding test documents:  87%|█████████████████████████████████████████████████████████████████████████████▍           | 174/200 [00:13<00:02, 12.46it/s]\u001b[A\n",
      "Encoding test documents:  88%|██████████████████████████████████████████████████████████████████████████████▎          | 176/200 [00:13<00:01, 12.52it/s]\u001b[A\n",
      "Encoding test documents:  90%|███████████████████████████████████████████████████████████████████████████████▋         | 179/200 [00:13<00:01, 13.92it/s]\u001b[A\n",
      "Encoding test documents:  90%|████████████████████████████████████████████████████████████████████████████████▌        | 181/200 [00:13<00:01, 14.00it/s]\u001b[A\n",
      "Encoding test documents:  92%|█████████████████████████████████████████████████████████████████████████████████▍       | 183/200 [00:13<00:01, 13.96it/s]\u001b[A\n",
      "Encoding test documents:  92%|██████████████████████████████████████████████████████████████████████████████████▎      | 185/200 [00:13<00:01, 13.52it/s]\u001b[A\n",
      "Encoding test documents:  94%|███████████████████████████████████████████████████████████████████████████████████▏     | 187/200 [00:13<00:00, 14.51it/s]\u001b[A\n",
      "Encoding test documents:  94%|████████████████████████████████████████████████████████████████████████████████████     | 189/200 [00:14<00:00, 14.83it/s]\u001b[A\n",
      "Encoding test documents:  96%|████████████████████████████████████████████████████████████████████████████████████▉    | 191/200 [00:14<00:00, 13.68it/s]\u001b[A\n",
      "Encoding test documents:  96%|█████████████████████████████████████████████████████████████████████████████████████▉   | 193/200 [00:14<00:00, 12.86it/s]\u001b[A\n",
      "Encoding test documents:  98%|██████████████████████████████████████████████████████████████████████████████████████▊  | 195/200 [00:14<00:00, 13.64it/s]\u001b[A\n",
      "Encoding test documents:  98%|███████████████████████████████████████████████████████████████████████████████████████▋ | 197/200 [00:14<00:00, 14.35it/s]\u001b[A\n",
      "Encoding test documents: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:14<00:00, 13.40it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "test_X = [bow.encode(d) for d in tqdm(test_documents, desc=\"Encoding test documents\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.vstack(train_X)\n",
    "test_X = np.vstack(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1800, 32404), (200, 32404))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVrGGArkrWoL"
   },
   "source": [
    "\n",
    "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
    "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
    "\n",
    "Use the held-out training set once again for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEGZ9SV8pPaa"
   },
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_z8sAJeUrdtM"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHWKDL3YV6vh"
   },
   "source": [
    "# Support Vector Machines (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJSYhcVaoJGt"
   },
   "source": [
    "Though simple to understand, implement, and debug, one\n",
    "major problem with the Naive Bayes classifier is that its performance\n",
    "deteriorates (becomes skewed) when it is being used with features which\n",
    "are not independent (i.e., are correlated). Another popular classifier\n",
    "that doesn’t scale as well to big data, and is not as simple to debug as\n",
    "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
    "Vector Machine (SVM) classifier.\n",
    "\n",
    "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
    "Other sources for learning SVM:\n",
    "* http://web.mit.edu/zoya/www/SVM.pdf\n",
    "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
    "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Use the scikit-learn implementation of \n",
    "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LnzNtQBV8gr"
   },
   "source": [
    "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
    "\n",
    "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
    "classification performance of the SVM classifier to that of the Naive\n",
    "Bayes classifier with smoothing.\n",
    "Use cross-validation to evaluate the performance of the classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "JBscui8Mvoz0"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "\n",
    "svc.fit(train_X, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(test_labels, svc.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifXVWcK0V9qY"
   },
   "source": [
    "### POS disambiguation (2pts)\n",
    "\n",
    "Now add in part-of-speech features. You will find the\n",
    "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
    "replicate the results obtained by Pang et al. (2002).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA3I82o4oWGu"
   },
   "source": [
    "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 1897, 1898, 1899])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = [reviews[i][\"content\"] for i in train_idx]\n",
    "train_labels = [reviews[i][\"sentiment\"] for i in train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BagOfWords(n_grams=[1], stemmer=stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Two', 'CD'],\n",
       " ['teen', 'JJ'],\n",
       " ['couples', 'NNS'],\n",
       " ['go', 'VBP'],\n",
       " ['to', 'TO'],\n",
       " ['a', 'DT'],\n",
       " ['church', 'NN'],\n",
       " ['party', 'NN'],\n",
       " [',', ','],\n",
       " ['drink', 'NN'],\n",
       " ['and', 'CC'],\n",
       " ['then', 'RB'],\n",
       " ['drive', 'NN'],\n",
       " ['.', '.']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_documents[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',_,': 1,\n",
       " '._.': 1,\n",
       " 'a_DT': 1,\n",
       " 'and_CC': 1,\n",
       " 'church_NN': 1,\n",
       " 'coupl_NNS': 1,\n",
       " 'drink_NN': 1,\n",
       " 'drive_NN': 1,\n",
       " 'go_VBP': 1,\n",
       " 'parti_NN': 1,\n",
       " 'teen_JJ': 1,\n",
       " 'then_RB': 1,\n",
       " 'to_TO': 1,\n",
       " 'two_CD': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.get_ngram_freq_dict([train_documents[0][0]], use_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 1,\n",
       " '.': 1,\n",
       " 'a': 1,\n",
       " 'and': 1,\n",
       " 'church': 1,\n",
       " 'coupl': 1,\n",
       " 'drink': 1,\n",
       " 'drive': 1,\n",
       " 'go': 1,\n",
       " 'parti': 1,\n",
       " 'teen': 1,\n",
       " 'then': 1,\n",
       " 'to': 1,\n",
       " 'two': 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.get_ngram_freq_dict([train_documents[0][0]], use_pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:11<00:00, 150.01it/s]                                                                      \n"
     ]
    }
   ],
   "source": [
    "bow.create_vocabulary(train_documents, train_labels, use_pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\", \"''\", \"'ll\", \"'re\", \"'s\", ',', '-lrb-', '-rrb-', '.', '102']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bow.class_wise_vocab[\"POS\"].keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:13<00:00, 131.65it/s]                                                                      \n"
     ]
    }
   ],
   "source": [
    "bow.create_vocabulary(train_documents, train_labels, use_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"''_''\",\n",
       " \"'_''\",\n",
       " \"'_POS\",\n",
       " \"'ll_MD\",\n",
       " \"'re_VBP\",\n",
       " \"'s_POS\",\n",
       " \"'s_VBZ\",\n",
       " ',_,',\n",
       " '-lrb-_-LRB-',\n",
       " '-rrb-_-RRB-']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bow.class_wise_vocab[\"POS\"].keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:04<00:00, 431.75it/s]                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with vocabulary of size 54555.\n",
      "Filtered vocabulary. Size of new vocabulary: 21540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 1800/1800 [00:07<00:00, 247.83it/s]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on train set: 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 200/200 [00:00<00:00, 244.29it/s]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on test set: 0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into training and testing data\n",
    "cv = np.array([reviews[i][\"cv\"] for i in range(len(reviews))])\n",
    "labels = np.array([reviews[i][\"sentiment\"] for i in range(len(reviews))])\n",
    "\n",
    "train_idx = np.where(cv < 900)[0]\n",
    "test_idx = np.where(cv >= 900)[0]\n",
    "\n",
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], filter_vocab=True, use_pos=True)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOvjYe-t2Br6"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0dt_oQupUNe"
   },
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su-3w87eMW0w"
   },
   "source": [
    "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filter_based_on_pos_tags(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221, 1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0][0]), len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "CCUPlPozCYUX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|████████████████████| 1800/1800 [00:01<00:00, 1456.10it/s]                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with vocabulary of size 26655.\n",
      "Filtered vocabulary. Size of new vocabulary: 10647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 1800/1800 [00:02<00:00, 657.24it/s]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on train set: 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████| 200/200 [00:00<00:00, 649.73it/s]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained accuracy using NaiveBayesClassifier on test set: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into training and testing data\n",
    "cv = np.array([reviews[i][\"cv\"] for i in range(len(reviews))])\n",
    "labels = np.array([reviews[i][\"sentiment\"] for i in range(len(reviews))])\n",
    "\n",
    "train_idx = np.where(cv < 900)[0]\n",
    "test_idx = np.where(cv >= 900)[0]\n",
    "\n",
    "clf = NaiveBayesClassifier(classes=[\"POS\", \"NEG\"], filter_vocab=True, use_pos=True)\n",
    "train_and_evaluate_clf(clf, train_idx, test_idx, filter_on_tags=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaxCVrs8pWSp"
   },
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfwqOciAl2No"
   },
   "source": [
    "# (Q4) Discussion (max. 500 words). (5pts)\n",
    "\n",
    "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
    "Why is this important? What are the limitations of these features and techniques?\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYuse5WLmekZ"
   },
   "source": [
    "*Write your answer here in up to 500 words (-0.25pt for >50 extra words, -0.5 points for >100 extra words, ...)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwaKwfWQhRk_"
   },
   "source": [
    "# Submission \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOUeaET5ijk-"
   },
   "outputs": [],
   "source": [
    "# Write your names and student numbers here:\n",
    "# Student 1 #12345\n",
    "# Student 2 #12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A9K-H6Tii3X"
   },
   "source": [
    "**That's it!**\n",
    "\n",
    "- Check if you answered all questions fully and correctly. \n",
    "- Download your completed notebook using `File -> Download .ipynb` \n",
    "- Check if your answers are all included in the file you submit.\n",
    "- Submit your .ipynb file via *Canvas*. One submission per group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHslatYAKBrF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP1 2021 Practical 1 (student version)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
