{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-aRiOgl4nHg"
      },
      "source": [
        "------\n",
        "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 POS and 1000 NEG **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys POS or\n",
        "NEG sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002). \n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**. \n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaZnxptMJiD7"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download). \n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects. \n",
        " \n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hok-BFu9lGoK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "#from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm-rakqtlMOT"
      },
      "outputs": [],
      "source": [
        "# download sentiment lexicon\n",
        "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "careEKj-mRpl"
      },
      "outputs": [],
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences, \n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "  \n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4: \n",
        "    break\n",
        "    \n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "      \n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "# Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., POS and NEG, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (POS, NEG, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogq0Eq2hQglh"
      },
      "outputs": [],
      "source": [
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  line_cnt = 0\n",
        "  for line in f:\n",
        "    print(line.strip())\n",
        "    line_cnt += 1\n",
        "    if line_cnt > 4:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a POS or a\n",
        "NEG label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more POS than NEG words per review (~7.13 more POS than NEG per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as POS.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{POS} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{NEG} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED2aTEYutW1-"
      },
      "outputs": [],
      "source": [
        "threshold = 8\n",
        "\n",
        "def get_polarity_lexicon(fp='sent_lexicon'):\n",
        "  lexicon = {}\n",
        "\n",
        "  polarity = {'positive': 1, 'negative': -1, 'neutral': 0, 'both': 0}\n",
        "\n",
        "  with open(fp, mode='r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      attrs = line.strip().split(' ')\n",
        "\n",
        "      attrs = {attr.split('=')[0]:  attr.split('=')[1] for attr in attrs}\n",
        "\n",
        "      lexicon[attrs['word1']] = attrs\n",
        "      lexicon[attrs['word1']]['priorpolarity'] = polarity[attrs['priorpolarity']]\n",
        "      \n",
        "  return lexicon\n",
        "\n",
        "sent_lexicon = get_polarity_lexicon()\n",
        "\n",
        "def classify_binary_score(review):\n",
        "  score = 0\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      if token.lower() in sent_lexicon:\n",
        "        score += sent_lexicon[token.lower()]['priorpolarity']\n",
        "        \n",
        "  return 'POS' if score - threshold > 0 else 'NEG'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy528EUTphz5"
      },
      "outputs": [],
      "source": [
        "# token_results should be a list of binary indicators; for example [1, 0, 1, ...] \n",
        "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
        "token_results = [1 if classify_binary_score(review) == review[\"sentiment\"] else 0 for review in reviews]\n",
        "token_accuracy = sum(token_results) / len(token_results)\n",
        "print(\"Accuracy: %0.2f\" % token_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG3hUDnPtkhS"
      },
      "outputs": [],
      "source": [
        "def calculate_weighted_bias(reviews, sent_lexicon):\n",
        "  bias = 0\n",
        "  for review in reviews:\n",
        "    for sentence in review[\"content\"]:\n",
        "      for token, pos_tag in sentence:\n",
        "        if token.lower() in sent_lexicon:\n",
        "          bias += sent_lexicon[token.lower()]['priorpolarity'] * (2 if sent_lexicon[token.lower()]['type'] == 'strongsubj' else 1)\n",
        "\n",
        "  \n",
        "  return bias / len(reviews)\n",
        "\n",
        "\n",
        "def classify_weighted_score(review, threshold):\n",
        "  score = 0\n",
        "  threshold = 8\n",
        "\n",
        "  total_weight = 0\n",
        "  recognized_words = 0\n",
        "\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "        if token.lower() in sent_lexicon:\n",
        "            weight = 2 if sent_lexicon[token.lower()]['type'] == 'strongsubj' else 1\n",
        "            total_weight += weight\n",
        "            recognized_words += 1\n",
        "  \n",
        "            score += sent_lexicon[token.lower()]['priorpolarity']  * weight\n",
        "\n",
        "  threshold = threshold * (total_weight / recognized_words)  \n",
        "\n",
        "  return 'POS' if score - threshold > 0 else 'NEG'\n",
        "\n",
        "\n",
        "weighted_bias = np.ceil(calculate_weighted_bias(reviews, sent_lexicon))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vVk7CvDpyka"
      },
      "outputs": [],
      "source": [
        "magnitude_results = [1 if classify_weighted_score(review, weighted_bias) == review[\"sentiment\"] else 0 for review in reviews]\n",
        "magnitude_accuracy = sum(magnitude_results) / len(magnitude_results)\n",
        "print(\"Accuracy: %0.2f\" % magnitude_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.4) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LgBcYcXsEk3"
      },
      "outputs": [],
      "source": [
        "plt.bar(['Token accuracy', 'Magnitude accuracy'], [token_accuracy, magnitude_accuracy])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.3) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more POS than NEG words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "The original threshold value was calculated by averaging over all the documents. This can cause problems when we have very short reviews. Consider a one sentence POS review: Since the threshold biases the classifier towards NEG results, the review may be seen as NEG because it has less than 8 POS words (it is hard to think of one sentence with more than 8 POS words in it!). While the one-sentence review is an edge case, this may be a problem for all reviews which are only a few sentences long, as a majority of the words in a review might be neutral or not display sentiment.\n",
        "\n",
        "A way to offset this problem would be to calculate the threshold in terms of the review length, compared to the average review length. If ~7.13 is the dataset bias, the bias for a particular review would be $7.13  * review\\_length / average\\_review\\_length$. This only requires the assumption that words with sentiment are uniformly distributed across text of varying length, which, while not perfect, should give a better bias for varying-size reviews. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwt0B8h8aKjr"
      },
      "outputs": [],
      "source": [
        "avg_review_length = sum(len(sentence) for review in reviews for sentence in review[\"content\"]) / len(reviews)\n",
        "\n",
        "def get_review_threshold(review):\n",
        "    return math.ceil(7.13 * sum(len(sentence) for sentence in review[\"content\"]) / avg_review_length) \n",
        "\n",
        "\n",
        "def classify_binary_score_var_threshold(review):\n",
        "  score = 0\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      if token.lower() in sent_lexicon:\n",
        "        score += sent_lexicon[token.lower()]['priorpolarity']\n",
        "  \n",
        "  threshold = get_review_threshold(review)\n",
        "  return 'POS' if score - threshold > 0 else 'NEG'\n",
        "\n",
        "\n",
        "var_threshold_results = [1 if classify_binary_score_var_threshold(review) == review[\"sentiment\"] else 0 for review in reviews]\n",
        "var_threshold_accuracy = sum(var_threshold_results) / len(var_threshold_results)\n",
        "print(\"Accuracy: %0.2f\" % var_threshold_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both POS and NEG training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes**.  What would be the problem instead with skipping words only for one class in case 2? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "Using the class-conditional probability of a word which has only appeared within one class in the training set would be uninformative and counter-productive. Suppose a word $w$ only appeared in POS reviews. When calculating the probability of a review being POS at prediction time, the $p(w|pos)$ term would only serve to decrease the likelihood of that class, and there is no corresponding $p(w|neg)$ to offset the influence of that word (when comparing with the probability of the other class). This would make us underestimate the probability of the POS class, which is not intuitively correct - we should assume that word $w$ is a good predictor of the POS class! (at least if we believe our training set to be large enough and representative of reviews in general)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (POS and NEG) reviews with cv-value 000-899, and test it on the remaining (POS and NEG) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7zaJYGFvIJ3"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def filter_reviews(reviews, pos_range, neg_range):\n",
        "    filtered_reviews = []\n",
        "\n",
        "    for review in reviews:\n",
        "        if review['sentiment'] == 'POS' and review['cv'] in pos_range:\n",
        "            filtered_reviews.append(review)\n",
        "        elif review['sentiment'] == 'NEG' and review['cv'] in neg_range:\n",
        "            filtered_reviews.append(review)\n",
        "    \n",
        "    return filtered_reviews\n",
        "\n",
        "\n",
        "def train_naive_bayes(reviews, smoothing=0, stemmer=None):\n",
        "    class_priors = defaultdict(float)\n",
        "    word_probabilities_pos = defaultdict(float)\n",
        "    word_probabilities_neg = defaultdict(float)\n",
        "\n",
        "    for review in reviews:\n",
        "        sentiment = review['sentiment']\n",
        "        \n",
        "        class_priors[sentiment] += 1\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for token, pos_tag in sentence:\n",
        "\n",
        "                token = token.lower()\n",
        "\n",
        "                if stemmer:\n",
        "                    token = stemmer.stem(token)\n",
        "\n",
        "                if sentiment == 'POS':\n",
        "                    word_probabilities_pos[token] += 1\n",
        "                else:\n",
        "                    word_probabilities_neg[token] += 1\n",
        "\n",
        "    if smoothing == 0:\n",
        "        \"\"\"\n",
        "        If not smoothing, remove words that occur in only one class.\n",
        "        \"\"\"\n",
        "        word_probabilities_pos = {word: count for word, count in word_probabilities_pos.items() if word in word_probabilities_neg}\n",
        "        word_probabilities_neg = {word: count for word, count in word_probabilities_neg.items() if word in word_probabilities_pos}\n",
        "    else:\n",
        "        \"\"\"\n",
        "        If smoothing, instantiate missing word counts in the classes to 0.\n",
        "        \"\"\"\n",
        "        for key, val in word_probabilities_neg.items():\n",
        "            if key not in word_probabilities_pos.keys():\n",
        "                word_probabilities_pos[key] = 0\n",
        "        \n",
        "        for key, val in word_probabilities_pos.items():\n",
        "            if key not in word_probabilities_neg.keys():\n",
        "                word_probabilities_neg[key] = 0\n",
        "\n",
        "    vocab_size = len(set(word_probabilities_pos.keys()))\n",
        "\n",
        "    pos_denom = sum(word_probabilities_pos.values()) + smoothing * vocab_size\n",
        "    neg_denom = sum(word_probabilities_neg.values()) + smoothing * vocab_size\n",
        "    \n",
        "    word_probabilities_pos = {word: np.log((word_probabilities_pos[word] + smoothing) / pos_denom) for word in word_probabilities_pos}\n",
        "    word_probabilities_neg = {word: np.log((word_probabilities_neg[word] + smoothing) / neg_denom) for word in word_probabilities_neg}\n",
        "\n",
        "    class_priors['POS'] = np.log( class_priors['POS'] / len(reviews) )\n",
        "    class_priors['NEG'] = np.log( class_priors['NEG'] / len(reviews) )\n",
        "\n",
        "    return class_priors, word_probabilities_pos, word_probabilities_neg\n",
        "\n",
        "\n",
        "def naive_bayes_clasifier(review, class_priors, word_probabilities_pos, word_probabilities_neg, stemmer=None):\n",
        "    pos_score = class_priors['POS']\n",
        "    neg_score = class_priors['NEG']\n",
        "\n",
        "    for sentence in review[\"content\"]:\n",
        "        for token, pos_tag in sentence:\n",
        "        \n",
        "            token = token.lower()\n",
        "\n",
        "            if stemmer:\n",
        "                token = stemmer.stem(token)\n",
        "\n",
        "            if token in word_probabilities_pos and token in word_probabilities_neg:\n",
        "                pos_score += word_probabilities_pos[token]\n",
        "                neg_score += word_probabilities_neg[token]\n",
        "\n",
        "    return 'POS' if pos_score > neg_score else 'NEG'\n",
        "\n",
        "training_set = filter_reviews(reviews, pos_range=range(0, 900, 1), neg_range=range(0, 900, 1))\n",
        "test_set = filter_reviews(reviews, pos_range=range(900, 1000, 1), neg_range=range(900, 1000, 1))\n",
        "\n",
        "class_priors, word_probabilities_pos, word_probabilities_neg = train_naive_bayes(training_set, smoothing=0)\n",
        "\n",
        "naive_bayes_results = [1 if naive_bayes_clasifier(review, class_priors,\n",
        "                                                 word_probabilities_pos,\n",
        "                                                  word_probabilities_neg)\n",
        "                            == \n",
        "                            review[\"sentiment\"]\n",
        "                       else 0 \n",
        "                       for review in test_set]\n",
        "naive_bayes_accuracy = sum(naive_bayes_results) / len(naive_bayes_results)\n",
        "print(\"Accuracy: %0.3f\" % naive_bayes_accuracy)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of POS movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the POS reviews\n",
        "data unchanged, but only using NEG reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbcsYlipBAw"
      },
      "source": [
        "Accuracy wouldn't be a good way to evaluate the classifier in this case since a classifier which always predicts POS sentiment would still have ~90% accuracy (which is what we see in the code below). In this case, it would be more informative to look at the confusion matrix, which gives a per-class breakdown of the classifier performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWDkt5ZrrFGp"
      },
      "outputs": [],
      "source": [
        "training_set_unbalanced = filter_reviews(reviews, pos_range=range(0, 900, 1), neg_range=range(0, 90, 1))\n",
        "test_set_unbalanced = filter_reviews(reviews, pos_range=range(900, 1000, 1) , neg_range=range(900, 910, 1))\n",
        "\n",
        "class_priors, word_probabilities_pos, word_probabilities_neg = train_naive_bayes(training_set_unbalanced, smoothing=0)\n",
        "\n",
        "naive_bayes_results = [1 if naive_bayes_clasifier(review, class_priors,\n",
        "                                                 word_probabilities_pos,\n",
        "                                                  word_probabilities_neg)\n",
        "                            == \n",
        "                            review[\"sentiment\"]\n",
        "                       else 0 \n",
        "                       for review in test_set_unbalanced]\n",
        "naive_bayes_accuracy = sum(naive_bayes_results) / len(naive_bayes_results)\n",
        "print(\"Accuracy: %0.2f\" % naive_bayes_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJzcHX3WUDm"
      },
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ for a word\n",
        "$w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the impact on performance. \n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g03yflCc9kpW"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "class_priors, word_probabilities_pos, word_probabilities_neg = train_naive_bayes(training_set, smoothing=1)\n",
        "\n",
        "naive_bayes_results = [1 if naive_bayes_clasifier(review, class_priors,\n",
        "                                                 word_probabilities_pos,\n",
        "                                                  word_probabilities_neg)\n",
        "                            == \n",
        "                            review[\"sentiment\"]\n",
        "                       else 0 \n",
        "                       for review in test_set]\n",
        "naive_bayes_accuracy = sum(naive_bayes_results) / len(naive_bayes_results)\n",
        "print(\"Accuracy: %0.2f\" % naive_bayes_accuracy)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Here we see the accuracy does not change significantly, likely because we are using a fair amount of reviews for training (900 per class). \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KeCGPa7Nuzx"
      },
      "outputs": [],
      "source": [
        "fold_ranges = [range(i, i + 900, 10) for i in range(10)]\n",
        "\n",
        "accuracies= []\n",
        "\n",
        "for test_fold_idx in range(10):\n",
        "    print('Training with fold {} left out'.format(test_fold_idx + 1))\n",
        "\n",
        "    training_set = []\n",
        "    test_set = []\n",
        "\n",
        "    for fold_idx, fold_range in enumerate(fold_ranges):\n",
        "        if fold_idx != test_fold_idx:\n",
        "            training_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "        else:\n",
        "            test_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "\n",
        "    class_priors, word_probabilities_pos, word_probabilities_neg = train_naive_bayes(training_set, smoothing=1)\n",
        "\n",
        "    naive_bayes_results = [1 if naive_bayes_clasifier(review, class_priors,\n",
        "                                                     word_probabilities_pos,\n",
        "                                                      word_probabilities_neg)\n",
        "                                == \n",
        "                                review[\"sentiment\"]\n",
        "                           else 0 \n",
        "                           for review in test_set]\n",
        "    naive_bayes_accuracy = sum(naive_bayes_results) / len(naive_bayes_results)\n",
        "    print(\"Accuracy: %0.2f\" % naive_bayes_accuracy)\n",
        "\n",
        "    accuracies.append(naive_bayes_accuracy)\n",
        "\n",
        "mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "print(\"Average Accuracy: %0.2f\" % mean_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoBQm1KuNzNR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Calculate the variance of the accuracies as a measure of how much the accuracy varies from the mean accuracy.\n",
        "\"\"\"\n",
        "\n",
        "variance = sum([(mean_accuracy - accuracy) ** 2 for accuracy in accuracies]) / len(accuracies)\n",
        "\n",
        "print(\"Variance: %0.7f\" % variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxtCul1IrBi_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We added a stemmer as an optional argument for both training and evaluation\n",
        "of the Naive Bayes algorithm. Here we just plug in the stemmer from the NLTK.\n",
        "\n",
        "(The performance of the algorithm is now quite a bit slower because of the stemmer.)\n",
        "\"\"\"\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for test_fold_idx in range(10):\n",
        "    print('Training with fold {} left out'.format(test_fold_idx + 1))\n",
        "\n",
        "    training_set = []\n",
        "    test_set = []\n",
        "\n",
        "    for fold_idx, fold_range in enumerate(fold_ranges):\n",
        "        if fold_idx != test_fold_idx:\n",
        "            training_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "        else:\n",
        "            test_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "\n",
        "    class_priors, word_probabilities_pos, word_probabilities_neg = train_naive_bayes(training_set, smoothing=1, stemmer=stemmer)\n",
        "\n",
        "    naive_bayes_results = [1 if naive_bayes_clasifier(review, class_priors,\n",
        "                                                     word_probabilities_pos,\n",
        "                                                      word_probabilities_neg, stemmer=stemmer)\n",
        "                                == \n",
        "                                review[\"sentiment\"]\n",
        "                           else 0 \n",
        "                           for review in test_set]\n",
        "    naive_bayes_accuracy = sum(naive_bayes_results) / len(naive_bayes_results)\n",
        "    print(\"Accuracy: %0.2f\" % naive_bayes_accuracy)\n",
        "\n",
        "    accuracies.append(naive_bayes_accuracy)\n",
        "\n",
        "mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "print(\"Average Accuracy: %0.2f\" % mean_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYqKBOiIrInT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Please see/run the cell above for results.\n",
        "We see a slight increase in accuracy using the stemmer in some of the folds,\n",
        "but the overall accuracy is slightly worse.\n",
        "\n",
        "This makes sense, as we are effectively reducing feature size. \n",
        "For example, third person verbs will likely not be used to express sentiment in reviews\n",
        "('Character X likes Y in this movie'), but first person forms will be used ('I liked Y in this movie').\n",
        "A stemmer will destroy this information.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA3vee5-rJyy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Our understanding is that we are using the held-out 'test set', not training set.\n",
        "The results would be similar for the training set, though the number of features\n",
        "would obviously rise as it is much bigger.\n",
        "\n",
        "The number of features with stemming will decrease.\n",
        "\"\"\"\n",
        "\n",
        "def get_vocab(reviews, stemmer=None):\n",
        "    vocab = {}\n",
        "    cnt = 0\n",
        "    for review in reviews:\n",
        "        for sentence in review['content']:\n",
        "            for token, pos_tag in sentence:\n",
        "                token = token.lower()\n",
        "                \n",
        "                if stemmer:\n",
        "                    token = stemmer.stem(token)\n",
        "                \n",
        "                if token not in vocab:\n",
        "                    vocab[token] = cnt\n",
        "                    cnt += 1\n",
        "    return vocab\n",
        "\n",
        "heldout_reviews = filter_reviews(reviews, pos_range=fold_ranges[0], neg_range=fold_ranges[0])\n",
        "\n",
        "print(\"Using a single fold to determine impact of stemmer on vocabulary size.\")\n",
        "print(\"Vocabulary size without stemmer: %d\" % len(get_vocab(heldout_reviews)))\n",
        "print(\"Vocabulary size with stemmer: %d\" % len(get_vocab(heldout_reviews, stemmer=stemmer)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYuKMTOpq9jz"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "def train_naive_bayes_ngrams(reviews, smoothing=1, stemmer=None, max_n=2):\n",
        "    \"\"\"\n",
        "    Train a Naive Bayes classifier on the given set of reviews.\n",
        "    \"\"\"\n",
        "    class_priors = defaultdict(float)\n",
        "    word_probabilities_pos = defaultdict(float)\n",
        "    word_probabilities_neg = defaultdict(float)\n",
        "\n",
        "\n",
        "    for review in reviews:\n",
        "        sentiment = review['sentiment']\n",
        "        class_priors[sentiment] += 1\n",
        "\n",
        "        for sentence in review['content']:\n",
        "            sentence = [t.lower() for t in [token for token in zip(*sentence)][0]]\n",
        "\n",
        "            if stemmer:\n",
        "                sentence = [stemmer.stem(token) for token in sentence]\n",
        "            \n",
        "            for n in range(1, max_n + 1):\n",
        "                for ngram in ngrams(sentence, n):\n",
        "                    ngram = tuple(ngram)\n",
        "                    if sentiment == 'POS':\n",
        "                        word_probabilities_pos[ngram] += 1\n",
        "                    else:\n",
        "                        word_probabilities_neg[ngram] += 1\n",
        "    \n",
        "    if smoothing == 0:\n",
        "        \"\"\"\n",
        "        If not smoothing, remove words that occur in only one class.\n",
        "        \"\"\"\n",
        "        word_probabilities_pos = {word: count for word, count in word_probabilities_pos.items() if word in word_probabilities_neg}\n",
        "        word_probabilities_neg = {word: count for word, count in word_probabilities_neg.items() if word in word_probabilities_pos}\n",
        "    else:\n",
        "        \"\"\"\n",
        "        If smoothing, instantiate missing word counts in the classes to 0.\n",
        "        \"\"\"\n",
        "        for key, val in word_probabilities_neg.items():\n",
        "            if key not in word_probabilities_pos.keys():\n",
        "                word_probabilities_pos[key] = 0\n",
        "        \n",
        "        for key, val in word_probabilities_pos.items():\n",
        "            if key not in word_probabilities_neg.keys():\n",
        "                word_probabilities_neg[key] = 0\n",
        "\n",
        "    vocab_size = len(set(word_probabilities_pos.keys()))\n",
        "\n",
        "    pos_denom = sum(word_probabilities_pos.values()) + smoothing * vocab_size\n",
        "    neg_denom = sum(word_probabilities_neg.values()) + smoothing * vocab_size\n",
        "    \n",
        "    word_probabilities_pos = {word: np.log((word_probabilities_pos[word] + smoothing) / pos_denom) for word in word_probabilities_pos}\n",
        "    word_probabilities_neg = {word: np.log((word_probabilities_neg[word] + smoothing) / neg_denom) for word in word_probabilities_neg}\n",
        "\n",
        "    class_priors['POS'] = np.log( class_priors['POS'] / len(reviews) )\n",
        "    class_priors['NEG'] = np.log( class_priors['NEG'] / len(reviews) )\n",
        "\n",
        "    return class_priors, word_probabilities_pos, word_probabilities_neg\n",
        "\n",
        "def naive_bayes_classifier_ngrams(review, class_priors, word_probabilities_pos, word_probabilities_neg, stemmer=None, max_n=2):\n",
        "\n",
        "    pos_score = class_priors['POS']\n",
        "    neg_score = class_priors['NEG']\n",
        "\n",
        "    for sentence in review['content']:\n",
        "        sentence = [t.lower() for t in [token for token in zip(*sentence)][0]]\n",
        "\n",
        "        if stemmer:\n",
        "            sentence = [stemmer.stem(token) for token in sentence]\n",
        "\n",
        "        for n in range(1, max_n + 1):\n",
        "            for ngram in ngrams(sentence, n):\n",
        "                ngram = tuple(ngram)\n",
        "\n",
        "                if ngram in word_probabilities_pos and ngram in word_probabilities_neg:\n",
        "                    pos_score += word_probabilities_pos[ngram]\n",
        "                    neg_score += word_probabilities_neg[ngram]\n",
        "\n",
        "    if pos_score > neg_score:\n",
        "        return 'POS'\n",
        "    else:\n",
        "        return 'NEG'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "training_set = filter_reviews(reviews, pos_range=range(0, 900, 1), neg_range=range(0, 900, 1))\n",
        "test_set = filter_reviews(reviews, pos_range=range(900, 1000, 1), neg_range=range(900, 1000, 1))\n",
        "\n",
        "for max_n in range(1, 4):\n",
        "    print('Training with up to {}-grams'.format(max_n))\n",
        "    accuracies = []\n",
        "    for test_fold_idx in range(10):\n",
        "\n",
        "        training_set = []\n",
        "        test_set = []\n",
        "\n",
        "        for fold_idx, fold_range in enumerate(fold_ranges):\n",
        "            if fold_idx != test_fold_idx:\n",
        "                training_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "            else:\n",
        "                test_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "\n",
        "        class_priors, word_probabilities_pos, word_probabilities_neg = train_naive_bayes_ngrams(training_set, smoothing=1, max_n=max_n)\n",
        "        naive_bayes_results = [1 if naive_bayes_classifier_ngrams(review, class_priors,\n",
        "                                                        word_probabilities_pos,\n",
        "                                                        word_probabilities_neg, max_n=max_n)\n",
        "                            == \n",
        "                            review[\"sentiment\"]\n",
        "                    else 0 \n",
        "                    for review in test_set]\n",
        "        naive_bayes_accuracy = sum(naive_bayes_results) / len(naive_bayes_results)\n",
        "        \n",
        "        print(\"Accuracy (fold {0:}): {1:.3g}\".format(test_fold_idx, naive_bayes_accuracy))\n",
        "\n",
        "        accuracies.append(naive_bayes_accuracy)\n",
        "\n",
        "    mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "    print(\"Average accuracy using up to {}-grams: {}\".format(max_n, mean_accuracy))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "We notice the accuracy does increase when using bigrams as features, but not when using trigrams.\n",
        "This is likely because trigrams are very sparse in the training set, and thus dilute the influence\n",
        "of more important features.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGZ9SV8pPaa"
      },
      "source": [
        "Since our features now include pairs (triplets) of words, we might expect their number to scale with a square (cube) in the number of distinct tokens. In reality, our corpus is limited, and some words appear very rarely (or never) together, so the actual number of features is always going to be quite a bit lower than the square (cube) of distinct tokens. We see that these numbers are 15436, 68182, 123648 for the held-out test set. So, the number of features increases roughly by an order of magnitude as we move from incorporating n-grams to n+1-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z8sAJeUrdtM"
      },
      "outputs": [],
      "source": [
        "open_class_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}\n",
        "\n",
        "\n",
        "def process_sentence(sentence, with_pos_tags=False, stemmer=None, open_class_only=False):\n",
        "\n",
        "    if open_class_only:\n",
        "        sentence = [(token, pos_tag) for (token, pos_tag) in sentence if pos_tag in open_class_pos_tags]\n",
        "\n",
        "        if len(sentence) == 0:\n",
        "            return []\n",
        "\n",
        "    if with_pos_tags:\n",
        "        tokens = [t.lower() for t in [token for token in zip(*sentence)][0]]\n",
        "\n",
        "        if stemmer:\n",
        "            tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        pos = [token for token in zip(*sentence)][1]\n",
        "\n",
        "        sentence = list(zip(tokens, pos))\n",
        "        \n",
        "    else:\n",
        "        sentence = [t.lower() for t in [token for token in zip(*sentence)][0]]\n",
        "\n",
        "        if stemmer:\n",
        "            sentence = [stemmer.stem(token) for token in sentence]\n",
        "\n",
        "    return sentence\n",
        "\n",
        "def get_features_ngrams(reviews, max_n=2, stemmer=None, with_pos_tags=False, open_class_only=False):\n",
        "    cnt = 0\n",
        "    features = dict()\n",
        "\n",
        "    for review in reviews:\n",
        "        for sentence in review['content']:\n",
        "\n",
        "            sentence = process_sentence(sentence, stemmer=stemmer,  with_pos_tags=with_pos_tags, open_class_only=open_class_only) \n",
        "            for n in range(1, max_n + 1):\n",
        "\n",
        "                for ngram in ngrams(sentence, n):\n",
        "                    if ngram not in features:\n",
        "                        features[ngram] = cnt\n",
        "                        cnt += 1\n",
        "\n",
        "    return features\n",
        "\n",
        "for max_n in range(1, 4):\n",
        "    print(\"Number of features using up to {}-grams: {}\".format(max_n, len(get_features_ngrams(heldout_reviews, max_n=max_n, with_pos_tags=True, open_class_only=True))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of \n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBscui8Mvoz0"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from scipy.sparse import dok_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "\n",
        "classifier = LinearSVC()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def get_feature_matrix(reviews, features, stemmer=None, with_pos_tags=False, open_class_only=False, _type='presence'):\n",
        "    feature_matrix = dok_matrix((len(reviews), len(features)), )\n",
        "\n",
        "    for i, review in enumerate(reviews):\n",
        "        for sentence in review['content']:\n",
        "\n",
        "\n",
        "            sentence = process_sentence(sentence, with_pos_tags=with_pos_tags, stemmer=stemmer, open_class_only=open_class_only)\n",
        "\n",
        "            for token in sentence:\n",
        "\n",
        "                token = (token, )\n",
        "\n",
        "                if token in features:\n",
        "\n",
        "                    if _type == 'presence':\n",
        "                        feature_matrix[i, features[token]] = 1\n",
        "                    elif _type == 'freq':\n",
        "                        feature_matrix[i, features[token]] += 1\n",
        "\n",
        "    #feature_matrix = normalize(feature_matrix, axis=1)\n",
        "\n",
        "    return feature_matrix\n",
        "\n",
        "\n",
        "\n",
        "fold_ranges = [range(i, i + 900, 10) for i in range(10)]\n",
        "\n",
        "accuracies= []\n",
        "\n",
        "for test_fold_idx in range(10):\n",
        "\n",
        "    training_set = []\n",
        "    test_set = []\n",
        "\n",
        "    for fold_idx, fold_range in enumerate(fold_ranges):\n",
        "        if fold_idx != test_fold_idx:\n",
        "            training_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "        else:\n",
        "            test_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "\n",
        "    features = get_features_ngrams(training_set, max_n=1)\n",
        "\n",
        "    classifier = LinearSVC()\n",
        "\n",
        "    training_matrix = get_feature_matrix(training_set, features, _type='freq').tocsr()\n",
        "    test_matrix = get_feature_matrix(test_set, features, _type='freq').tocsr()\n",
        "\n",
        "    classifier.fit(training_matrix, [1 if review['sentiment'] == 'POS' else -1 for review in training_set])\n",
        "\n",
        "    svm_predictions_test = ['POS' if prediction == 1 else 'NEG' for prediction in classifier.predict(test_matrix)] \n",
        "    svm_accuracy_test = sum([1 if prediction == review['sentiment'] else 0 for prediction, review in zip(svm_predictions_test, test_set)])/ len(test_set)\n",
        "\n",
        "\n",
        "    print(\"Accuracy (fold {0:}): {1:.3g}\".format(test_fold_idx, svm_accuracy_test))\n",
        "\n",
        "    accuracies.append(svm_accuracy_test)\n",
        "\n",
        "mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "print(\"Average Accuracy: %0.2f\" % mean_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOvjYe-t2Br6"
      },
      "outputs": [],
      "source": [
        "fold_ranges = [range(i, i + 900, 10) for i in range(10)]\n",
        "\n",
        "accuracies= []\n",
        "\n",
        "for test_fold_idx in range(10):\n",
        "\n",
        "    training_set = []\n",
        "    test_set = []\n",
        "\n",
        "    for fold_idx, fold_range in enumerate(fold_ranges):\n",
        "        if fold_idx != test_fold_idx:\n",
        "            training_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "        else:\n",
        "            test_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "\n",
        "    features = get_features_ngrams(training_set, max_n=1)\n",
        "\n",
        "    classifier = LinearSVC()\n",
        "\n",
        "    features = get_features_ngrams(training_set, max_n=1, with_pos_tags=True)\n",
        "\n",
        "    training_matrix_POS = get_feature_matrix(training_set, features, with_pos_tags=True, _type='freq').tocsr()\n",
        "    test_matrix_POS = get_feature_matrix(test_set, features, with_pos_tags=True, _type='freq').tocsr()\n",
        "\n",
        "    classifier.fit(training_matrix_POS, [1 if review['sentiment'] == 'POS' else -1 for review in training_set])\n",
        "\n",
        "    svm_prediction_POS = ['POS' if prediction == 1 else 'NEG' for prediction in classifier.predict(test_matrix_POS)]\n",
        "    svm_accuracy_POS = sum([1 if prediction == review['sentiment'] else 0 for prediction, review in zip(svm_prediction_POS, test_set)])/ len(test_set)\n",
        "\n",
        "    print(\"Accuracy (fold {0:}): {1:.3g}\".format(test_fold_idx, svm_accuracy_POS))\n",
        "\n",
        "    accuracies.append(svm_accuracy_POS)\n",
        "\n",
        "mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "print(\"Average Accuracy: %0.2f\" % mean_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "POS information in principle helps because it allows us to differentiate between words which would otherwise be seen as equivalent. 'love' as a verb is more indicative of sentiment than 'love' as a noun. In our case, the accuracy of the SVM does not seem to increase much when including the POS-tags, but this could be a limitation of the classifier (and different kernels, hyperparameters, etc. might exploit this new information better)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCUPlPozCYUX"
      },
      "outputs": [],
      "source": [
        "fold_ranges = [range(i, i + 900, 10) for i in range(10)]\n",
        "\n",
        "accuracies= []\n",
        "\n",
        "for test_fold_idx in range(10):\n",
        "\n",
        "    training_set = []\n",
        "    test_set = []\n",
        "\n",
        "    for fold_idx, fold_range in enumerate(fold_ranges):\n",
        "        if fold_idx != test_fold_idx:\n",
        "            training_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "        else:\n",
        "            test_set += filter_reviews(reviews, pos_range=fold_range, neg_range=fold_range)\n",
        "\n",
        "    classifier = LinearSVC()\n",
        "\n",
        "    features = get_features_ngrams(training_set, max_n=1, with_pos_tags=True, open_class_only=True)\n",
        "\n",
        "    training_matrix_POS = get_feature_matrix(training_set, features, with_pos_tags=True, open_class_only=True, _type='freq').tocsr()\n",
        "    test_matrix_POS = get_feature_matrix(test_set, features, with_pos_tags=True, open_class_only=True, _type='freq').tocsr()\n",
        "\n",
        "    classifier.fit(training_matrix_POS, [1 if review['sentiment'] == 'POS' else -1 for review in training_set])\n",
        "\n",
        "    svm_prediction_POS = ['POS' if prediction == 1 else 'NEG' for prediction in classifier.predict(test_matrix_POS)]\n",
        "    svm_accuracy_POS = sum([1 if prediction == review['sentiment'] else 0 for prediction, review in zip(svm_prediction_POS, test_set)])/ len(test_set)\n",
        "\n",
        "    print(\"Accuracy (fold {0:}): {1:.3g}\".format(test_fold_idx, svm_accuracy_POS))\n",
        "\n",
        "    accuracies.append(svm_accuracy_POS)\n",
        "\n",
        "mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "print(\"Average Accuracy: %0.2f\" % mean_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaxCVrs8pWSp"
      },
      "source": [
        "Closed-class words include token which are not indicative of sentiment and which will generally be present in most reviews ('in', 'a', 'the', '.'). Since we are using a bag of words approach, the presence of these words is not informative and may dilute the influence of other, more informative words, hence hindering the classification process. Again, we do not see an appreciable change in the overall accuracy of the SVM, probably because throughout its training, it learns to focus on the more informative features, and make decisions based on them, so manually removing 'useless' features does not change results a lot. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (Q4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "From our experiments, Naive Bayes and SVM obtain similar results when it comes to sentiment classification, with accuracies a bit over 80%. The simpler, lexicon-based approaches prove to be too crude and only obtain accuracies up to 70%. \n",
        "\n",
        "The base features we have been using for all the classifiers are word-counts in a review (i.e. bag-of-word modelling). This kind of modelling has the inherent limitation that it cannot distinguish between 'Although I hate the main character, I love this movie' and 'Although I love the main character, I hate this movie.', since finer semantic content of the reviews is lost. \n",
        "\n",
        "However, even a crude approach such as the lexicon-based algorithms does provide results significantly better than chance, just by summing over the 'polarity' of the words apparent in the reviews. In this case, the polarities are defined by human hand, and we don't leverage probabilistic reasoning. The Naive Bayes classifier does leverage probabilistic reasoning, though it makes the simplyfying assumption of class-conditional word independence. Still, it is able to draw more information from the features, and learns directly from the data the 'polarity' of the individual words, which proves to be more robust than the human-supervised approach. Incorporating bigrams as features allows us to capture some of the word order in the reviews (e.g. the classifier can now distinguish between 'I love' and 'don't love'), and it leads to an increase in the classifier accuracy. While incorporating trigrams and higher-order ngrams sounds like a good idea, it leads to a feature explosion, and as trigrams/fourgrams/etc. are very sparsely appearing in the texts, they do not add to the discriminative power of the classifiers.\n",
        "\n",
        "SVMs show consistently slightly better performance than the Naive Bayes classifier, likely because it is able to implicitly capture correlation between different features. Adding POS-tags as features and removing closed-class words seems to result in slightly better performance (even than Naive Bayes with bigrams). This is expected since we can differentiate between nouns/verb/adjectives that may be identical as words.\n",
        "\n",
        "In the paper by Pang et al. it is noted that the SVM performs better when the features used are just presence-indicators of words as opposed to word counts in a review. We experimented with this and noticed that indeed the SVM performs a bit better using the 'presence' approach, pushing the accuracy to about 86%. However, our increase in performance was not as substantial as the one in the paper. There are too many points of variability to ascertain why this is exactly the case (different SVM algorithm, different dataset, etc.) \n",
        "\n",
        "Note: The SVM can be run in presence mode by chaing the _type argument to 'presence', as opposed to 'freq' (count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwaKwfWQhRk_"
      },
      "source": [
        "# Submission \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOUeaET5ijk-"
      },
      "outputs": [],
      "source": [
        "# Write your names and student numbers here:\n",
        "# Tadija Radusinovic #13704222\n",
        "# Student 2 #12345"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9K-H6Tii3X"
      },
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly. \n",
        "- Download your completed notebook using `File -> Download .ipynb` \n",
        "- Check if your answers are all included in the file you submit.\n",
        "- Submit your .ipynb file via *Canvas*. One submission per group. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHslatYAKBrF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP1 2021 Practical 1 (student version)",
      "provenance": []
    },
    "interpreter": {
      "hash": "58d433cb776384629783f7f687c778d76306c4cc431c4ad273e9c8ad57a1d750"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
